; This is an example configuration for training a language model. For a more detailed
; description of an INI example, please refer to the translation.ini file

[main]
name="language modeling"
output="out-example-langmodel"
tf_manager=<tf_manager>

train_dataset=<train_data>
val_dataset=<val_data>
test_datasets=[<val_data>]

runners=[<runner>]
trainer=<trainer>
evaluation=[("perplexity", "words", <perplexity>)]

batch_size=50
epochs=50

validation_period=500
logging_period=20

[perplexity]
; Dummy evaluator that just averages the numbers returned
class=evaluators.average.AverageEvaluator
name="perplexity"

[tf_manager]
class=tf_manager.TensorFlowManager
num_sessions=1
num_threads=4
minimize_metric=True


# DATA

[train_data]
class=dataset.load_dataset_from_files
s_words="examples/data/language_model/train"
preprocessors=[("words", "words_char", processors.helpers.preprocess_char_based)]

[val_data]
class=dataset.load_dataset_from_files
s_words="examples/data/language_model/val"
preprocessors=[("words", "words_char", processors.helpers.preprocess_char_based)]

[vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["words_char"]
max_size=1000


# MODEL

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[]
rnn_size=512
embedding_size=512
data_id="words_char"
vocabulary=<vocabulary>
max_output_len=50


# TRAINERS & RUNNERS

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
l2_weight=1.0e-8
clip_norm=1.0

[runner]
class=runners.perplexity_runner.PerplexityRunner
decoder=<decoder>
output_series="perplexity"
